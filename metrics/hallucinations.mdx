---
title: "Hallucinations"
description: "Overview of Hallucinations Detections"
---

## How do we define hallucinations?

The **hallucination rate** measures how often a model generates information that cannot be found in its provided inputs, such as retrieved documents, user messages, or system prompts. Specifically, Quotient reports an **_extrinsic hallucination rate_**: we determine whether the model’s output is externally unsupported by the context it was given.

<Tip>
  ### What is an _Extrinsic_ Hallucination?

  Extrinsic hallucinations occur when a model generates content that is not backed by any input. This is distinct from **intrinsic hallucinations**, where the model generates text that is self-contradictory or logically incoherent regardless of the input.

  We focus on **extrinsic** hallucination detection, because this is what matters most in retrieval-augmented systems: **does the model stick to the facts it was given?**\
  \
  Refer to [How to Detect Hallucinations in Retrieval Augmented Systems: A Primer](https://blog.quotientai.co/how-to-detect-hallucinations-in-retrieval-augmented-systems-a-primer/) for an in-depth overview of hallucinations in augmented AI systems.
</Tip>

## How do we detect hallucinations?

1. **Break output into individual claims or sentences**
2. **Compare each claim to available context inputs**, including:
   - `documents` (retrieved evidence)
   - `user_input` (end-user query or message)
   - `message_history` (prior turns in the conversation)
3. **Flag claims that lack support** in any of the above inputs

If a sentence cannot be traced back to a supported input, it's counted as a hallucination.

## Why it's important to monitor your AI system for hallucinations?

Extrinsic hallucinations are the primary failure mode in augmented AI systems. Even when retrieval succeeds, generation can drift. This metric helps teams:

- Catch hallucinations early in development
- Monitor output quality post-deployment
- Guide prompt iteration and model fine-tuning

Well-grounded systems typically show \< 5% hallucination rate. If yours is higher, it's often a signal that either your retrieval pipeline or prompting needs improvement.